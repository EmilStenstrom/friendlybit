---
comments:
- comment_ID: '638'
  comment_author: Sarven Capadisli
  comment_author_url: http://www.csarven.ca
  comment_content: Concerning:\n# Assisting search engines\n\nI am not sure how effective
    the lang attribute is. \n\nA while back I had a problem with this in fact. \nI've
    placed lang="en" on my pages, however google had a problem choosing the wrong
    language for one of my articles. \n\nThe only reasoning I could come up with was
    that the article (at the time) was  more popular in the German (de) community.\n\nSo
    I had to write this:\n<a href="http://www.csarven.ca/google-deutschland-stole-my-article"
    rel="nofollow">Google Deutschland stole my article</a>\n\nAlthough this is no
    longer an issue, I am still spectacle when it comes to search engines (in fact
    anything) if they are really acknowledging the language of a given document.
  comment_date: '2006-05-14 01:23:50'
  comment_post_ID: '63'
  comment_type: null
  is_admin: '0'
- comment_ID: '647'
  comment_author: Steve Tucker
  comment_author_url: http://www.stevetucker.co.uk
  comment_content: I try to always put some reference to the language in my markup
    documents - it makes logical sense. At worst cannot do any harm!
  comment_date: '2006-05-15 00:28:45'
  comment_post_ID: '63'
  comment_type: null
  is_admin: '0'
- comment_ID: '654'
  comment_author: Jesse Skinner
  comment_author_url: http://www.thefutureoftheweb.com/
  comment_content: Thanks for the algorithm.. I would never have known where to begin
    to implement something like this.\n\nI think, maybe dictionary files would be
    the perfect place to build up a bigram statistic list. Those are usually (somewhat)
    easy to find.
  comment_date: '2006-05-15 16:25:33'
  comment_post_ID: '63'
  comment_type: null
  is_admin: '0'
- comment_ID: '667'
  comment_author: Emil Stenström
  comment_author_url: http://friendlybit.com
  comment_content: '@Jesse Skinner: I wouldn''t use a dictionary. Since the texts
    you will be testing are ordinary texts you should base your text data on that
    kind of texts. Say if English contains a lot of "in". Then that bigram should
    have a high percentage, whether the reason is because it''s in many different
    words or that those words are common.'
  comment_date: '2006-05-16 09:46:04'
  comment_post_ID: '63'
  comment_type: null
  is_admin: '1'
- comment_ID: '1490'
  comment_author: Adam Zakreski
  comment_author_url: ''
  comment_content: I find your statement, "In fact, most of the content online is
    not in English." A little hard to swallow.  Even with the citation, the original
    source seems to only be refering to a certain type of blog posts.  I believe saying,
    "most blog content online is not in English," would be more accurate (though still
    a stretch).  I do find it a lot easier to believe that the Japanese are more avid
    bloggers than English speakers, though.
  comment_date: '2006-06-16 23:54:32'
  comment_post_ID: '63'
  comment_type: null
  is_admin: '0'
- comment_ID: '1653'
  comment_author: Emil Stenström
  comment_author_url: http://friendlybit.com
  comment_content: '@Adam Zakreski: The link indeed handles only blog content that
    technorati indexes. <a href="http://ausweb.scu.edu.au/aw03/papers/edwards2/paper.html"
    rel="nofollow">Other sources</a> have managed to approximate the number to 68%
    which means you are right there. Well done :)\n\n<a href="http://global-reach.biz/globstats/index.php3"
    rel="nofollow">Global Reach</a> meassures the number of people online by language
    and finds 35%, something that tells much about the future of the web.'
  comment_date: '2006-06-19 12:36:20'
  comment_post_ID: '63'
  comment_type: null
  is_admin: '1'
- comment_ID: '3994'
  comment_author: Dan Pettersson
  comment_author_url: ''
  comment_content: Does anyone have an example of code (e.g. in PHP) that would compare
    two texts and give you a number...\n\nIn my own tests squaring and square rooting
    messes things up. E.g. if I duplicate a text it won't show the same result.\n\nI've
    solved it by making my own version without the squares, and I think that works
    fine...
  comment_date: '2006-08-27 00:02:59'
  comment_post_ID: '63'
  comment_type: null
  is_admin: '0'
- comment_ID: '4010'
  comment_author: Emil Stenström
  comment_author_url: http://friendlybit.com
  comment_content: '@Dan Pettersson: so your algorithm gives different numbers for
    the exact same texts? That''s some bug in your code. If you post a link to it
    (<a href="http://se2.php.net/highlight_file" rel="nofollow">syntax highlight it</a>)
    and I''ll have a look.'
  comment_date: '2006-08-27 10:56:57'
  comment_post_ID: '63'
  comment_type: null
  is_admin: '1'
- comment_ID: '22298'
  comment_author: Peter Vigren
  comment_author_url: ''
  comment_content: I am also interested in example code for PHP. I find this quite
    intriguing, maybe because I find programming AND languages so fascinating. :-)
    But one thing, if you remove or translate characters like é etc, won't that make
    the result a bit weird? After all, if a language uses é a lot and others don't,
    shouldn't that be significant? In any case, I really enjoyed this article, first
    time I ever had heard of this so it tickles my brain like crazy! Thank you. :-)
  comment_date: '2007-03-11 17:33:12'
  comment_post_ID: '63'
  comment_type: null
  is_admin: '0'
- comment_ID: '22388'
  comment_author: Emil Stenström
  comment_author_url: http://friendlybit.com
  comment_content: '@Peter Vigren: I''ll see what I can do, I''m moving atm so not
    too much extra time available.'
  comment_date: '2007-03-15 23:14:12'
  comment_post_ID: '63'
  comment_type: null
  is_admin: '1'
---